{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy import misc, ndimage\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    " def get_files(filename):\n",
    "     class_train = []\n",
    "     label_train = []\n",
    "     for train_class in os.listdir(filename):\n",
    "         for pic in os.listdir(filename+train_class):\n",
    "             class_train.append(filename+train_class+'/'+pic)\n",
    "         label_train.append(train_class)\n",
    "         temp = np.array([class_train,label_train])\n",
    "         temp = temp.transpose()\n",
    "     #shuffle the samples\n",
    "         np.random.shuffle(temp)\n",
    "    #after transpose, images is in dimension 0 and label in dimension 1\n",
    "         image_list = list(temp[:,0])\n",
    "         label_list = list(temp[:,1])\n",
    "         label_list = [int(i) for i in label_list]\n",
    "     #print(label_list)\n",
    "         return image_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    " def get_batches(image,label,resize_w,resize_h,batch_size,capacity):\n",
    "      #convert the list of images and labels to tensor\n",
    "      image = tf.cast(image,tf.string)\n",
    "      label = tf.cast(label,tf.int64)\n",
    "      queue = tf.train.slice_input_producer([image,label])\n",
    "      label = queue[1]\n",
    "      image_c = tf.read_file(queue[0])\n",
    "      image = tf.image.decode_jpeg(image_c,channels = 3)\n",
    "     #resize\n",
    "      image = tf.image.resize_image_with_crop_or_pad(image,resize_w,resize_h)\n",
    "     #(x - mean) / adjusted_stddev\n",
    "      image = tf.image.per_image_standardization(image)\n",
    "     \n",
    "      image_batch,label_batch = tf.train.batch([image,label],\n",
    "                                              batch_size = batch_size,\n",
    "                                          num_threads = 64,\n",
    "                                             capacity = capacity)\n",
    "      images_batch = tf.cast(image_batch,tf.float32)\n",
    "      labels_batch = tf.reshape(label_batch,[batch_size])\n",
    "      return images_batch,labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "  def init_weights(shape):\n",
    "      return tf.Variable(tf.random_normal(shape,stddev = 0.01))\n",
    "  #init weights\n",
    "  weights = {\n",
    "     \"w1\":init_weights([3,3,3,16]),\n",
    "     \"w2\":init_weights([3,3,16,128]),\n",
    "     \"w3\":init_weights([3,3,128,256]),\n",
    "     \"w4\":init_weights([4096,4096]),\n",
    "     \"wo\":init_weights([4096,2])\n",
    "     }\n",
    " \n",
    " #init biases\n",
    "  biases = {\n",
    "    \"b1\":init_weights([16]),\n",
    "     \"b2\":init_weights([128]),\n",
    "     \"b3\":init_weights([256]),\n",
    "     \"b4\":init_weights([4096]),\n",
    "     \"bo\":init_weights([2])\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def conv2d(x,w,b):\n",
    "      x = tf.nn.conv2d(x,w,strides = [1,1,1,1],padding = \"SAME\")\n",
    "      x = tf.nn.bias_add(x,b)\n",
    "      return tf.nn.relu(x)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def pooling(x):\n",
    "      return tf.nn.max_pool(x,ksize = [1,2,2,1],strides = [1,2,2,1],padding = \"SAME\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def norm(x,lsize = 4):\n",
    "      return tf.nn.lrn(x,depth_radius = lsize,bias = 1,alpha = 0.001/9.0,beta = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def mmodel(images):\n",
    "      l1 = conv2d(images,weights[\"w1\"],biases[\"b1\"])\n",
    "      l2 = pooling(l1)\n",
    "      l2 = norm(l2)\n",
    "      l3 = conv2d(l2,weights[\"w2\"],biases[\"b2\"])\n",
    "      l4 = pooling(l3)\n",
    "      l4 = norm(l4)\n",
    "      l5 = conv2d(l4,weights[\"w3\"],biases[\"b3\"])\n",
    "     #same as the batch size\n",
    "      l6 = pooling(l5)\n",
    "      l6 = tf.reshape(l6,[-1,weights[\"w4\"].get_shape().as_list()[0]])\n",
    "      l7 = tf.nn.relu(tf.matmul(l6,weights[\"w4\"])+biases[\"b4\"])\n",
    "      soft_max = tf.add(tf.matmul(l7,weights[\"wo\"]),biases[\"bo\"])\n",
    "      return soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    " def loss(logits,label_batches):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=label_batches)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(logits,labels):\n",
    "     acc = tf.nn.in_top_k(logits,labels,1)\n",
    "     acc = tf.cast(acc,tf.float32)\n",
    "     acc = tf.reduce_mean(acc)\n",
    "     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def training(loss,lr):\n",
    "     train_op = tf.train.RMSPropOptimizer(lr,0.9).minimize(loss)\n",
    "     return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "      data_dir = '/Users/zoro/Desktop/tensorflowcd/tf_files/'\n",
    "      image,label = inputData.get_files(data_dir)\n",
    "      image_batches,label_batches = inputData.get_batches(image,label,32,32,16,20)\n",
    "      p = model.mmodel(image_batches)\n",
    "      cost = model.loss(p,label_batches)\n",
    "      train_op = model.training(cost,0.001)\n",
    "      acc = model.get_accuracy(p,label_batches)\n",
    "      \n",
    "      sess = tf.Session()\n",
    "      init = tf.global_variables_initializer()\n",
    "      sess.run(init)\n",
    "     \n",
    "      coord = tf.train.Coordinator()\n",
    "      threads = tf.train.start_queue_runners(sess = sess,coord = coord)\n",
    "     \n",
    "      try:\n",
    "         for step in np.arange(1000):\n",
    "             print(step)\n",
    "             if coord.should_stop():\n",
    "                 break\n",
    "             _,train_acc,train_loss = sess.run([train_op,acc,cost])\n",
    "             print(\"loss:{} accuracy:{}\".format(train_loss,train_acc))\n",
    "      except tf.errors.OutOfRangeError:\n",
    "          print(\"Done!!!\")\n",
    "      finally:\n",
    "          coord.request_stop()\n",
    "      coord.join(threads)\n",
    "      sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    " data_dir = '/Users/zoro/Desktop/tensorflowcd/tf_files/'\n",
    " log_dir = '/Users/zoro/Desktop/tensorflowcd/tf_files/log/'\n",
    " image,label = inputData.get_files(data_dir)\n",
    " image_batches,label_batches = inputData.get_batches(image,label,32,32,16,20)\n",
    " print(image_batches.shape)\n",
    " p = model.mmodel(image_batches,16)\n",
    " cost = model.loss(p,label_batches)\n",
    " train_op = model.training(cost,0.001)\n",
    " acc = model.get_accuracy(p,label_batches)\n",
    "  \n",
    " sess = tf.Session()\n",
    " init = tf.global_variables_initializer()\n",
    " sess.run(init)\n",
    " saver = tf.train.Saver()\n",
    " coord = tf.train.Coordinator()\n",
    " threads = tf.train.start_queue_runners(sess = sess,coord = coord)\n",
    "  \n",
    " try:\n",
    "  for step in np.arange(1000):\n",
    "   print(step)\n",
    "   if coord.should_stop():\n",
    "    break\n",
    "   _,train_acc,train_loss = sess.run([train_op,acc,cost])\n",
    "   print(\"loss:{} accuracy:{}\".format(train_loss,train_acc))\n",
    "   if step % 100 == 0:\n",
    "    check = os.path.join(log_dir,\"model.ckpt\")\n",
    "    saver.save(sess,check,global_step = step)\n",
    " except tf.errors.OutOfRangeError:\n",
    "  print(\"Done!!!\")\n",
    " finally:\n",
    "  coord.request_stop()\n",
    " coord.join(threads)\n",
    " sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def mmodel(images,batch_size):\n",
    " with tf.variable_scope('conv1') as scope:\n",
    "  weights = tf.get_variable('weights', \n",
    "         shape = [3,3,3, 16],\n",
    "         dtype = tf.float32, \n",
    "         initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "  biases = tf.get_variable('biases', \n",
    "         shape=[16],\n",
    "         dtype=tf.float32,\n",
    "         initializer=tf.constant_initializer(0.1))\n",
    "  conv = tf.nn.conv2d(images, weights, strides=[1,1,1,1], padding='SAME')\n",
    "  pre_activation = tf.nn.bias_add(conv, biases)\n",
    "  conv1 = tf.nn.relu(pre_activation, name= scope.name)\n",
    " with tf.variable_scope('pooling1_lrn') as scope:\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1],strides=[1,2,2,1],\n",
    "        padding='SAME', name='pooling1')\n",
    "  norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "       beta=0.75,name='norm1')\n",
    " with tf.variable_scope('conv2') as scope:\n",
    "  weights = tf.get_variable('weights',\n",
    "         shape=[3,3,16,128],\n",
    "         dtype=tf.float32,\n",
    "         initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "  biases = tf.get_variable('biases',\n",
    "         shape=[128], \n",
    "         dtype=tf.float32,\n",
    "         initializer=tf.constant_initializer(0.1))\n",
    "  conv = tf.nn.conv2d(norm1, weights, strides=[1,1,1,1],padding='SAME')\n",
    "  pre_activation = tf.nn.bias_add(conv, biases)\n",
    "  conv2 = tf.nn.relu(pre_activation, name='conv2') \n",
    " with tf.variable_scope('pooling2_lrn') as scope:\n",
    "  norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "       beta=0.75,name='norm2')\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1,2,2,1], strides=[1,1,1,1],\n",
    "        padding='SAME',name='pooling2')\n",
    " with tf.variable_scope('local3') as scope:\n",
    "      reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "      dim = reshape.get_shape()[1].value\n",
    "      weights = tf.get_variable('weights',\n",
    "         shape=[dim,4096],\n",
    "         dtype=tf.float32,\n",
    "         initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "      biases = tf.get_variable('biases',\n",
    "         shape=[4096],\n",
    "         dtype=tf.float32, \n",
    "         initializer=tf.constant_initializer(0.1))\n",
    "      local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name) \n",
    " with tf.variable_scope('softmax_linear') as scope:\n",
    "      weights = tf.get_variable('softmax_linear',\n",
    "         shape=[4096, 2],\n",
    "         dtype=tf.float32,\n",
    "         initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "      biases = tf.get_variable('biases', \n",
    "         shape=[2],\n",
    "         dtype=tf.float32, \n",
    "         initializer=tf.constant_initializer(0.1))\n",
    "      softmax_linear = tf.add(tf.matmul(local3, weights), biases, name='softmax_linear')\n",
    "      return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_one_image(img_dir):\n",
    "    image = Image.open(img_dir)\n",
    "    plt.imshow(image)\n",
    "    image = image.resize([32, 32])\n",
    "    image_arr = np.array(image)\n",
    "    return image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(test_file):\n",
    " log_dir = '/Users/zoro/Desktop/tensorflowcd/tf_files/log/'\n",
    " image_arr = get_one_image(test_file)\n",
    "  \n",
    " with tf.Graph().as_default():\n",
    "  image = tf.cast(image_arr, tf.float32)\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  image = tf.reshape(image, [1,32, 32, 3])\n",
    "  print(image.shape)\n",
    "  p = model.mmodel(image,1)\n",
    "  logits = tf.nn.softmax(p)\n",
    "  x = tf.placeholder(tf.float32,shape = [32,32,3])\n",
    "  saver = tf.train.Saver()\n",
    "  with tf.Session() as sess:\n",
    "   ckpt = tf.train.get_checkpoint_state(log_dir)\n",
    "   if ckpt and ckpt.model_checkpoint_path:\n",
    "    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print('Loading success')\n",
    "   else:\n",
    "    print('No checkpoint')\n",
    "   prediction = sess.run(logits, feed_dict={x: image_arr})\n",
    "   max_index = np.argmax(prediction)\n",
    "   print(max_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
